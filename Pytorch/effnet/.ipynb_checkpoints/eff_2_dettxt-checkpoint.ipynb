{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f71c9c-53b3-4b06-a836-d3f0aad9bc0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Créer une video à partir de la séquence de frames sous img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60fefad-9b39-42c4-b069-9e32d741c075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import configparser\n",
    "import glob\n",
    "\n",
    "# Fonction pour traiter une séquence individuelle\n",
    "def process_sequence(path_seq, video_output):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(os.path.join(path_seq, 'seqinfo.ini'))\n",
    "\n",
    "    sequence_info = config['Sequence']\n",
    "\n",
    "    im_dir = sequence_info['imDir']\n",
    "    frame_rate = int(sequence_info['frameRate'])\n",
    "    seq_length = int(sequence_info['seqLength'])\n",
    "    im_width = int(sequence_info['imWidth'])\n",
    "    im_height = int(sequence_info['imHeight'])\n",
    "    im_ext = sequence_info['imExt']\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(video_output, fourcc, frame_rate, (im_width, im_height))\n",
    "\n",
    "    image_files = sorted(os.listdir(path_seq+'/'+im_dir))\n",
    "\n",
    "    frames_processed = 0\n",
    "\n",
    "    for image_file in image_files:\n",
    "        if frames_processed >= seq_length:\n",
    "            break\n",
    "\n",
    "        if image_file.endswith(im_ext):\n",
    "            image_path = os.path.join(path_seq+'/'+im_dir, image_file)\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is not None:\n",
    "                video_writer.write(img)\n",
    "                frames_processed += 1\n",
    "            else:\n",
    "                print(f\"Erreur lors de la lecture de l'image : {image_path}\")\n",
    "\n",
    "    video_writer.release()\n",
    "\n",
    "\n",
    "# Chemin du répertoire contenant les séquences MOT17\n",
    "mot17_path = '/home/jovyan/iadatasets/MOT/MOT17/train/'\n",
    "output_dir = '/home/jovyan/Desktop/mot_videos/'\n",
    "\n",
    "# Liste des séquences FRCNN\n",
    "sequences = glob.glob(os.path.join(mot17_path, \"*-FRCNN\"))\n",
    "\n",
    "# Traiter chaque séquence FRCNN et sauvegarder la vidéo dans le répertoire de sortie\n",
    "for seq in sequences:\n",
    "    seq_name = os.path.basename(seq)\n",
    "    video_output = os.path.join(output_dir, f\"{seq_name}.mp4\")\n",
    "    process_sequence(seq, video_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f415ae5-eef1-43ea-a49c-ae4d2f5a53ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76bc31a1-dbd7-4ff3-a918-95fcde4561a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Afficher les informations de la vidéo (faut surtout vérifier le nbre de frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c497f49-fea4-472c-b6f5-55e45b8563f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Ouvrir la vidéo\n",
    "video_output='/home/jovyan/Desktop/MOT_VIDEOS/MOT17-09-FRCNN.mp4'\n",
    "video = cv2.VideoCapture(video_output)\n",
    "\n",
    "# Récupérer les informations de la vidéo\n",
    "frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_rate = int(video.get(cv2.CAP_PROP_FPS))\n",
    "video_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "duration = frame_count / frame_rate\n",
    "\n",
    "# Afficher les informations de la vidéo\n",
    "print(f\"Nombre de frames: {frame_count}\")\n",
    "print(f\"Frame rate: {frame_rate} FPS\")\n",
    "print(f\"Largeur: {video_width}\")\n",
    "print(f\"Hauteur: {video_height}\")\n",
    "print(f\"Durée: {duration:.2f} secondes\")\n",
    "\n",
    "# Libérer les ressources\n",
    "video.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45478465-da79-46c4-b7a0-73d3d11cd30a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Créer un file det.txt à partir des détections de efficientdet.pth appliqué à une vidéo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf6944a-f6b9-40eb-ba6f-277966206359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_detections_to_file(det_results, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for frame_id, detections in det_results:\n",
    "            for det in detections:\n",
    "                # Les coordonnées de la boîte englobante\n",
    "                bb_left = det[0]\n",
    "                bb_top = det[1]\n",
    "                bb_width = det[2] - det[0]\n",
    "                bb_height = det[3] - det[1]\n",
    "                # La confiance de la détection\n",
    "                conf = det[4]\n",
    "                # L'id de l'objet est indéterminé; -1\n",
    "                obj_id = -1\n",
    "                # Les coordonnées 3D sont indéterminées, donc on met -1\n",
    "                x = y = z = -1\n",
    "                # Écrire la détection dans le fichier\n",
    "                f.write(f\"{frame_id},-1,{bb_left},{bb_top},{bb_width},{bb_height},{conf},{x},{y},{z}\\n\")\n",
    "             #   print(bb_left,bb_top,bb_width,bb_height,conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b183a51b-111a-44dc-a32d-e076d4517c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from backbone import EfficientDetBackbone\n",
    "from efficientdet.utils import BBoxTransform, ClipBoxes\n",
    "from utils.utils import preprocess, invert_affine, postprocess, preprocess_video_frame,preprocess_video\n",
    "import torch\n",
    "import time\n",
    "\n",
    "compound_coef = 1\n",
    "input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n",
    "use_cuda = True\n",
    "use_float16 = False\n",
    "gpu = 0\n",
    "threshold = 0.2\n",
    "nms_threshold = 0.1\n",
    "params = {\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225],\n",
    "    'anchors_scales': '[2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]',\n",
    "    'anchors_ratios': '[(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)]',\n",
    "        }\n",
    "video_path='/home/jovyan/Desktop/MOT_VIDEOS/MOT17-04-FRCNN.mp4'\n",
    "\n",
    "\n",
    "# Charger le mod  le\n",
    "model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=90)\n",
    "state_dict = torch.load('/home/jovyan/Desktop/Pytorch/weights/efficientdet-d1.pth')\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.eval()\n",
    "\n",
    "# Utiliser le GPU si n  cessaire\n",
    "if use_cuda:\n",
    "    model.cuda(gpu)\n",
    "    if use_float16:\n",
    "        model.half()\n",
    "\n",
    "# Initialiser les objets pour la post-processing\n",
    "regressBoxes = BBoxTransform()\n",
    "clipBoxes = ClipBoxes()\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_id = 1\n",
    "det_results = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "     # Pr  traiter l'image\n",
    "    ori_img, framed_imgs, framed_meta = preprocess_video(frame, max_size=input_sizes[compound_coef],\n",
    "                                                         mean=params['mean'], std=params['std'])\n",
    "   # print('ori_img.shape',ori_img.shape)\n",
    "  #  print('framed_imgs',framed_imgs)\n",
    "  #  print('framed_meta',framed_meta)\n",
    "\n",
    "    if use_cuda:\n",
    "        x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n",
    "    else:\n",
    "        x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n",
    "\n",
    "    x = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n",
    "\n",
    "    # Passer l'image à travers EfficientDet\n",
    "  #  print('x.shape',x.shape)\n",
    "  #  print('CECI X input to model',x)\n",
    "    features, regression, classification, anchors = model(x)\n",
    "\n",
    "    # Post-traiter les predictions\n",
    "    preds = postprocess(x,\n",
    "                        anchors, regression, classification,\n",
    "                        regressBoxes, clipBoxes,\n",
    "                        threshold, nms_threshold)\n",
    "\n",
    "    if preds:\n",
    "        # Inverser la transformation appliquée à l'image\n",
    "        preds = invert_affine(framed_meta, preds)[0]\n",
    "        #print('preds',preds)\n",
    "        # Récupérer les détections\n",
    "        scores = preds['scores']\n",
    "        class_ids = preds['class_ids']\n",
    "        rois = preds['rois']\n",
    "   #     print('rois',rois)\n",
    "       # print('scores',scores)\n",
    "        #la liste des détections pour ce frame\n",
    "        det_results.append((frame_id, [[int(x1), int(y1), int(x2), int(y2), score] for (x1, y1, x2, y2), score in zip(rois.tolist(), scores.tolist())]))\n",
    "        \n",
    "    frame_id += 1\n",
    "   # break\n",
    "# Enregistrer les détections dans un fichier\n",
    "save_detections_to_file(det_results, 'output.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44782190-9a93-4afb-a1ac-11b41f3a7984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44a5ac62-84e4-4d31-917e-7e158d074e74",
   "metadata": {},
   "source": [
    "### feature pkl file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b76831b-6120-440d-8636-5f5670a1b807",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'eff'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbackbone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EfficientDetBackbone\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Paramètres\u001b[39;00m\n\u001b[1;32m      9\u001b[0m compound_coef \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Pytorch/script/backbone.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meff\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mefficientdet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BiFPN, Regressor, Classifier, EfficientNet\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meff\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mefficientdet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Anchors\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEfficientDetBackbone\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'eff'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from backbone import EfficientDetBackbone\n",
    "\n",
    "# Paramètres\n",
    "compound_coef = 1\n",
    "input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n",
    "use_cuda = True\n",
    "use_float16 = False\n",
    "gpu = 0\n",
    "params = {\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225],\n",
    "}\n",
    "image_dir = '/iadatasets/MOT/MOT17/train/MOT17-02-FRCNN/img1'\n",
    "output_dir = '/home/jovyan/Desktop/feature/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Charger le modèle\n",
    "model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=90)\n",
    "state_dict = torch.load('/home/jovyan/Desktop/Pytorch/weights/efficientdet-d1.pth')\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.eval()\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda(gpu)\n",
    "    if use_float16:\n",
    "        model.half()\n",
    "\n",
    "# Boucle sur toutes les images\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.endswith('.jpg'):  # ajuster l'extension du fichier si nécessaire\n",
    "        # Charger l'image\n",
    "        image = cv2.imread(os.path.join(image_dir, filename))\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        # Prétraiter l'image\n",
    "        height, width = image.shape[:2]\n",
    "        image_size = input_sizes[compound_coef]\n",
    "        scale = min(image_size / height, image_size / width)\n",
    "        image = cv2.resize(image, (int(scale * width), int(scale * height)))\n",
    "        image = image.astype(np.float32) / 255\n",
    "        image -= params['mean']\n",
    "        image /= params['std']\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "\n",
    "        # Passer l'image à travers EfficientDet\n",
    "        with torch.no_grad():\n",
    "            if use_cuda:\n",
    "                image = torch.from_numpy(image).cuda().float()\n",
    "            else:\n",
    "                image = torch.from_numpy(image).float()\n",
    "            features, _, _, _ = model(image)\n",
    "\n",
    "        # Sauvegarder les caractéristiques dans un fichier .pkl\n",
    "        feature_file = os.path.join(output_dir, filename + '.pkl')\n",
    "        with open(feature_file, 'wb') as f:\n",
    "            pickle.dump(features.cpu().numpy(), f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
