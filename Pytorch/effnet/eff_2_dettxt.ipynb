{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f71c9c-53b3-4b06-a836-d3f0aad9bc0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Créer une video à partir de la séquence de frames sous img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60fefad-9b39-42c4-b069-9e32d741c075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import configparser\n",
    "import glob\n",
    "\n",
    "# Fonction pour traiter une séquence individuelle\n",
    "def process_sequence(path_seq, video_output):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(os.path.join(path_seq, 'seqinfo.ini'))\n",
    "\n",
    "    sequence_info = config['Sequence']\n",
    "\n",
    "    im_dir = sequence_info['imDir']\n",
    "    frame_rate = int(sequence_info['frameRate'])\n",
    "    seq_length = int(sequence_info['seqLength'])\n",
    "    im_width = int(sequence_info['imWidth'])\n",
    "    im_height = int(sequence_info['imHeight'])\n",
    "    im_ext = sequence_info['imExt']\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(video_output, fourcc, frame_rate, (im_width, im_height))\n",
    "\n",
    "    image_files = sorted(os.listdir(path_seq+'/'+im_dir))\n",
    "\n",
    "    frames_processed = 0\n",
    "\n",
    "    for image_file in image_files:\n",
    "        if frames_processed >= seq_length:\n",
    "            break\n",
    "\n",
    "        if image_file.endswith(im_ext):\n",
    "            image_path = os.path.join(path_seq+'/'+im_dir, image_file)\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is not None:\n",
    "                video_writer.write(img)\n",
    "                frames_processed += 1\n",
    "            else:\n",
    "                print(f\"Erreur lors de la lecture de l'image : {image_path}\")\n",
    "\n",
    "    video_writer.release()\n",
    "\n",
    "\n",
    "# Chemin du répertoire contenant les séquences MOT17\n",
    "mot17_path = '/home/jovyan/iadatasets/MOT/MOT17/train/'\n",
    "output_dir = '/home/jovyan/Desktop/mot_videos/'\n",
    "\n",
    "# Liste des séquences FRCNN\n",
    "sequences = glob.glob(os.path.join(mot17_path, \"*-FRCNN\"))\n",
    "\n",
    "# Traiter chaque séquence FRCNN et sauvegarder la vidéo dans le répertoire de sortie\n",
    "for seq in sequences:\n",
    "    seq_name = os.path.basename(seq)\n",
    "    video_output = os.path.join(output_dir, f\"{seq_name}.mp4\")\n",
    "    process_sequence(seq, video_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f415ae5-eef1-43ea-a49c-ae4d2f5a53ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76bc31a1-dbd7-4ff3-a918-95fcde4561a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Afficher les informations de la vidéo (faut surtout vérifier le nbre de frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c497f49-fea4-472c-b6f5-55e45b8563f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Ouvrir la vidéo\n",
    "video_output='/home/jovyan/Desktop/MOT_VIDEOS/MOT17-09-FRCNN.mp4'\n",
    "video = cv2.VideoCapture(video_output)\n",
    "\n",
    "# Récupérer les informations de la vidéo\n",
    "frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_rate = int(video.get(cv2.CAP_PROP_FPS))\n",
    "video_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "duration = frame_count / frame_rate\n",
    "\n",
    "# Afficher les informations de la vidéo\n",
    "print(f\"Nombre de frames: {frame_count}\")\n",
    "print(f\"Frame rate: {frame_rate} FPS\")\n",
    "print(f\"Largeur: {video_width}\")\n",
    "print(f\"Hauteur: {video_height}\")\n",
    "print(f\"Durée: {duration:.2f} secondes\")\n",
    "\n",
    "# Libérer les ressources\n",
    "video.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45478465-da79-46c4-b7a0-73d3d11cd30a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Créer un file det.txt à partir des détections de efficientdet.pth appliqué à une vidéo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbf6944a-f6b9-40eb-ba6f-277966206359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_detections_to_file(det_results, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for frame_id, detections in det_results:\n",
    "            for det in detections:\n",
    "                # Les coordonnées de la boîte englobante\n",
    "                bb_left = det[0]\n",
    "                bb_top = det[1]\n",
    "                bb_width = det[2] - det[0]\n",
    "                bb_height = det[3] - det[1]\n",
    "                # La confiance de la détection\n",
    "                conf = det[4]\n",
    "                # L'id de l'objet est indéterminé; -1\n",
    "                obj_id = -1\n",
    "                # Les coordonnées 3D sont indéterminées, donc on met -1\n",
    "                x = y = z = -1\n",
    "                # Écrire la détection dans le fichier\n",
    "                f.write(f\"{frame_id},-1,{bb_left},{bb_top},{bb_width},{bb_height},{conf},{x},{y},{z}\\n\")\n",
    "             #   print(bb_left,bb_top,bb_width,bb_height,conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b183a51b-111a-44dc-a32d-e076d4517c05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_size torch.Size([12, 12])\n",
      "combined_feature initial tensor([[[[ 2.7530e-01, -4.5979e-01,  3.7629e-01,  ...,  4.5612e-01,\n",
      "            7.2212e-01,  4.7469e-01],\n",
      "          [-3.6784e-01,  4.5565e-02,  2.1484e-01,  ...,  4.1473e-01,\n",
      "            1.0398e+00,  3.4476e-02],\n",
      "          [ 1.4328e+00,  6.8675e-01, -5.0410e-01,  ...,  9.9381e-01,\n",
      "            2.5067e-01,  2.2186e-01],\n",
      "          ...,\n",
      "          [-3.0292e-02,  1.0623e+00, -9.7127e-01,  ..., -1.4080e-01,\n",
      "           -1.2552e+00, -1.5818e-01],\n",
      "          [ 4.7368e-02, -6.4292e-01, -5.6723e-01,  ...,  2.8653e-01,\n",
      "           -1.1101e+00,  6.4429e-01],\n",
      "          [ 6.8558e-01, -3.0504e-02, -5.1785e-01,  ..., -3.3316e-01,\n",
      "           -1.7179e+00, -4.5952e-01]],\n",
      "\n",
      "         [[-1.7163e-01, -3.3242e-02,  3.3638e-01,  ..., -6.5499e-01,\n",
      "           -4.8732e-01,  6.4229e-01],\n",
      "          [ 2.3364e-01,  6.8937e-01,  2.0814e-01,  ..., -1.5072e+00,\n",
      "           -1.7049e+00,  6.4442e-01],\n",
      "          [-6.0212e-01, -8.1162e-01, -1.1984e+00,  ..., -1.0669e+00,\n",
      "           -2.4951e-01,  1.9015e-01],\n",
      "          ...,\n",
      "          [ 2.2124e+00, -1.4365e+00,  6.5627e-01,  ..., -8.0536e-01,\n",
      "            5.6501e-01,  7.2425e-01],\n",
      "          [ 1.0276e+00,  1.0299e-01,  1.4999e+00,  ...,  5.2555e-01,\n",
      "            7.2234e-01, -1.8609e-01],\n",
      "          [ 2.3142e-01, -9.8383e-01, -5.5052e-01,  ..., -4.2352e-01,\n",
      "           -1.3526e+00,  7.7606e-01]],\n",
      "\n",
      "         [[ 1.3850e+00,  7.7783e-01, -4.7650e-01,  ...,  1.1483e+00,\n",
      "            1.9876e+00,  6.0556e-01],\n",
      "          [ 7.3849e-01,  4.0536e-01,  6.3551e-01,  ...,  6.5547e-01,\n",
      "            2.0025e+00,  2.5238e-01],\n",
      "          [ 2.7459e-01,  9.0915e-01,  2.3630e+00,  ...,  4.8680e-01,\n",
      "            1.5880e+00,  1.4289e+00],\n",
      "          ...,\n",
      "          [-8.1183e-01, -2.6872e-01, -1.6801e-01,  ..., -1.7968e-01,\n",
      "           -3.4891e-01,  4.6830e-01],\n",
      "          [-1.4211e+00,  8.3308e-02, -4.5703e-01,  ..., -2.5327e-01,\n",
      "           -2.1982e+00, -3.7386e-02],\n",
      "          [-1.0139e+00, -3.3375e-01,  5.0952e-02,  ..., -3.3008e-01,\n",
      "           -1.0666e+00, -9.6949e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7682e-02,  4.5199e-02,  1.0023e-01,  ...,  4.9389e-02,\n",
      "            4.8459e-02,  4.7994e-02],\n",
      "          [-2.7686e-02, -1.5487e-03,  5.0725e-02,  ...,  3.2624e-02,\n",
      "            3.2724e-02,  3.2774e-02],\n",
      "          [-1.1842e-01, -9.5044e-02, -4.8289e-02,  ..., -9.0508e-04,\n",
      "            1.2550e-03,  2.3350e-03],\n",
      "          ...,\n",
      "          [ 2.4709e-02,  5.9251e-02,  1.2833e-01,  ..., -1.9659e-01,\n",
      "           -1.8171e-01, -1.7426e-01],\n",
      "          [ 2.8926e-02,  9.7951e-02,  2.3600e-01,  ..., -2.1725e-01,\n",
      "           -1.8777e-01, -1.7303e-01],\n",
      "          [ 3.1034e-02,  1.1730e-01,  2.8984e-01,  ..., -2.2757e-01,\n",
      "           -1.9080e-01, -1.7242e-01]],\n",
      "\n",
      "         [[ 7.0095e-02,  1.5687e-01,  3.3041e-01,  ...,  3.6704e-02,\n",
      "            1.7527e-02,  7.9381e-03],\n",
      "          [ 7.0425e-02,  1.6295e-01,  3.4801e-01,  ...,  5.3713e-02,\n",
      "            2.4431e-02,  9.7896e-03],\n",
      "          [ 7.1085e-02,  1.7512e-01,  3.8320e-01,  ...,  8.7731e-02,\n",
      "            3.8239e-02,  1.3493e-02],\n",
      "          ...,\n",
      "          [-2.4613e-01, -2.0445e-01, -1.2108e-01,  ..., -3.4748e-01,\n",
      "           -3.2751e-01, -3.1752e-01],\n",
      "          [-3.9836e-01, -3.6014e-01, -2.8368e-01,  ..., -3.9506e-01,\n",
      "           -3.6601e-01, -3.5148e-01],\n",
      "          [-4.7448e-01, -4.3798e-01, -3.6499e-01,  ..., -4.1885e-01,\n",
      "           -3.8526e-01, -3.6847e-01]],\n",
      "\n",
      "         [[ 3.6927e-01,  2.7732e-01,  9.3415e-02,  ..., -5.4735e-02,\n",
      "           -1.1408e-02,  1.0256e-02],\n",
      "          [ 3.8925e-01,  2.8532e-01,  7.7462e-02,  ..., -9.8264e-02,\n",
      "           -4.8660e-02, -2.3858e-02],\n",
      "          [ 4.2921e-01,  3.0133e-01,  4.5557e-02,  ..., -1.8532e-01,\n",
      "           -1.2316e-01, -9.2085e-02],\n",
      "          ...,\n",
      "          [ 3.1809e-01,  3.1354e-01,  3.0446e-01,  ...,  5.1021e-02,\n",
      "           -1.3920e-02, -4.6391e-02],\n",
      "          [ 6.6827e-02,  1.2011e-01,  2.2667e-01,  ...,  2.8965e-02,\n",
      "           -2.8255e-02, -5.6865e-02],\n",
      "          [-5.8802e-02,  2.3392e-02,  1.8778e-01,  ...,  1.7938e-02,\n",
      "           -3.5422e-02, -6.2101e-02]]]], device='cuda:0')\n",
      "rois [[1695.1195    385.026    1871.9902    728.73724 ]\n",
      " [1287.5664    462.9792   1356.2603    657.4972  ]\n",
      " [  13.440604  484.6553    121.88055   886.32825 ]\n",
      " [1414.1515    525.1397   1494.6223    642.1546  ]\n",
      " [ 252.08147   461.86047   335.63538   700.2447  ]\n",
      " [1789.4385    468.43976  1875.9033    576.58405 ]\n",
      " [  40.819748  346.67578   236.18114   426.8985  ]\n",
      " [ 122.77414   498.1391    203.26265   742.33154 ]\n",
      " [1329.2914    510.1128   1365.0775    578.8427  ]\n",
      " [  44.085968  566.65594   109.403275  653.2107  ]\n",
      " [ 414.4145    495.2686    427.7991    534.40546 ]\n",
      " [1886.5054    383.13382  1915.7852    574.3492  ]\n",
      " [ 508.64398   501.40826   528.8008    556.66675 ]]\n",
      "scores [0.8880802  0.6517805  0.49034074 0.45751485 0.45344308 0.4498249\n",
      " 0.32200643 0.31569415 0.24738988 0.21762446 0.21202414 0.20851171\n",
      " 0.20237821]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cropped_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 103\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m#la liste des détections pour ce frame\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     det_results\u001b[38;5;241m.\u001b[39mappend((frame_id, [[\u001b[38;5;28mint\u001b[39m(x1), \u001b[38;5;28mint\u001b[39m(y1), \u001b[38;5;28mint\u001b[39m(x2), \u001b[38;5;28mint\u001b[39m(y2), score] \u001b[38;5;28;01mfor\u001b[39;00m (x1, y1, x2, y2), score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(rois\u001b[38;5;241m.\u001b[39mtolist(), scores\u001b[38;5;241m.\u001b[39mtolist())]))\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcropped_features\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[43mcropped_features\u001b[49m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcropped_features\u001b[39m\u001b[38;5;124m'\u001b[39m,cropped_features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    106\u001b[0m frame_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cropped_features' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from backbone import EfficientDetBackbone\n",
    "from efficientdet.utils import BBoxTransform, ClipBoxes\n",
    "from utils.utils import preprocess, invert_affine, postprocess, preprocess_video_frame,preprocess_video\n",
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "compound_coef = 2\n",
    "input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n",
    "use_cuda = True\n",
    "use_float16 = False\n",
    "gpu = 0\n",
    "threshold = 0.2\n",
    "nms_threshold = 0.1\n",
    "params = {\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225],\n",
    "    'anchors_scales': '[2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]',\n",
    "    'anchors_ratios': '[(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)]',\n",
    "        }\n",
    "video_path='/home/jovyan/Desktop/MOT_VIDEOS/MOT17-09-FRCNN.mp4'\n",
    "\n",
    "\n",
    "# Charger le mod  le\n",
    "model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=90)\n",
    "state_dict = torch.load('/home/jovyan/Desktop/Pytorch/weights/efficientdet-d2.pth')\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.eval()\n",
    "\n",
    "# Utiliser le GPU si n  cessaire\n",
    "if use_cuda:\n",
    "    model.cuda(gpu)\n",
    "    if use_float16:\n",
    "        model.half()\n",
    "\n",
    "# Initialiser les objets pour la post-processing\n",
    "regressBoxes = BBoxTransform()\n",
    "clipBoxes = ClipBoxes()\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_id = 1\n",
    "det_results = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "     # Pr  traiter l'image\n",
    "    ori_img, framed_imgs, framed_meta = preprocess_video(frame, max_size=input_sizes[compound_coef],\n",
    "                                                         mean=params['mean'], std=params['std'])\n",
    "   # print('ori_img.shape',ori_img.shape)\n",
    "  #  print('framed_imgs',framed_imgs)\n",
    "  #  print('framed_meta',framed_meta)\n",
    "\n",
    "    if use_cuda:\n",
    "        x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n",
    "    else:\n",
    "        x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n",
    "\n",
    "    x = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n",
    "\n",
    "    # Passer l'image à travers EfficientDet\n",
    "    features, regression, classification, anchors = model(x)\n",
    "    target_size = features[3].shape[-2:] \n",
    "    print('target_size',target_size)\n",
    "    # On fait un interpolate (upsampling) pour chaque feature map pour atteindre la taille voulue, puis on les concatène\n",
    "    resized_features = [F.interpolate(f, size=target_size, mode='bilinear', align_corners=False) for f in features]\n",
    "    combined_feature = torch.cat(resized_features, dim=1)\n",
    "    print('combined_feature initial',combined_feature)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    # Post-traiter les predictions\n",
    "    preds = postprocess(x,\n",
    "                        anchors, regression, classification,\n",
    "                        regressBoxes, clipBoxes,\n",
    "                        threshold, nms_threshold)\n",
    "\n",
    "    if preds:\n",
    "        # Inverser la transformation appliquée à l'image\n",
    "        preds = invert_affine(framed_meta, preds)[0]\n",
    "        #print('preds',preds)\n",
    "        # Récupérer les détections\n",
    "        scores = preds['scores']\n",
    "        class_ids = preds['class_ids']\n",
    "        rois = preds['rois']\n",
    "        print('rois',rois)\n",
    "        print('scores',scores)\n",
    "        #la liste des détections pour ce frame\n",
    "        \n",
    "        det_results.append((frame_id, [[int(x1), int(y1), int(x2), int(y2), score] for (x1, y1, x2, y2), score in zip(rois.tolist(), scores.tolist())]))\n",
    "\n",
    "    print('cropped_features',cropped_features)\n",
    "\n",
    "    print('cropped_features',cropped_features.shape)\n",
    "    frame_id += 1\n",
    "    break\n",
    "    \n",
    "    \n",
    "    \n",
    "# Enregistrer les détections dans un fichier\n",
    "save_detections_to_file(det_results, 'output.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2861f0dd-1a0f-4ad6-ac5e-b52405d8cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import roi_align\n",
    "\n",
    "class RoIAlign(nn.Module):\n",
    "    def __init__(self, crop_height, crop_width, transform_fpcoor=True, sampling_ratio=-1):\n",
    "        super(RoIAlign, self).__init__()\n",
    "\n",
    "        self.crop_height = crop_height\n",
    "        self.crop_width = crop_width\n",
    "        self.transform_fpcoor = transform_fpcoor\n",
    "        self.sampling_ratio = sampling_ratio\n",
    "\n",
    "    def forward(self, featuremap, boxes, box_ind):\n",
    "        boxes = boxes.detach().contiguous()  # detacher du calcul du gradient,\n",
    "        box_ind = box_ind.detach()\n",
    "        return roi_align(featuremap, boxes, (self.crop_height, self.crop_width), sampling_ratio=self.sampling_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44782190-9a93-4afb-a1ac-11b41f3a7984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44a5ac62-84e4-4d31-917e-7e158d074e74",
   "metadata": {},
   "source": [
    "### feature pkl file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76831b-6120-440d-8636-5f5670a1b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from backbone import EfficientDetBackbone\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Paramètres\n",
    "compound_coef = 1\n",
    "input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n",
    "use_cuda = True\n",
    "use_float16 = False\n",
    "gpu = 0\n",
    "params = {\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225],\n",
    "}\n",
    "image_dir = '/home/jovyan/iadatasets/MOT/MOT17/train/MOT17-02-FRCNN/img1'\n",
    "output_dir = '/home/jovyan/Desktop/feature/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Charger le modèle\n",
    "model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=90)\n",
    "state_dict = torch.load('/home/jovyan/Desktop/Pytorch/weights/efficientdet-d1.pth')\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.eval()\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda(gpu)\n",
    "    if use_float16:\n",
    "        model.half()\n",
    "\n",
    "# Boucle sur toutes les images\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.endswith('.jpg'): \n",
    "        # Charger l'image\n",
    "        image = cv2.imread(os.path.join(image_dir, filename))\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        # Taille d'entrée pour EfficientDet-d1\n",
    "        image_size = input_sizes[compound_coef]  # i.e., 640\n",
    "\n",
    "        # Resize\n",
    "        image = cv2.resize(image, (image_size, image_size))\n",
    "        image = image.astype(np.float32) / 255\n",
    "        image -= params['mean']\n",
    "        image /= params['std']\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "\n",
    "        # Passer l'image à travers EfficientDet\n",
    "        with torch.no_grad():\n",
    "            if use_cuda:\n",
    "                image = torch.from_numpy(image).cuda().float()\n",
    "            else:\n",
    "                image = torch.from_numpy(image).float()\n",
    "            features, _, _, _ = model(image)\n",
    "        #    print('features[0] shape',features[0].shape)\n",
    "        #    print('features[0] shape',features[1].shape)\n",
    "        #    print('features[0] shape',features[2].shape)\n",
    "        #    print('features[0] shape',features[3].shape)\n",
    "        #    print('features[0] shape',features[4].shape)\n",
    "            \n",
    "            # On détermine la taille cible comme la taille de la plus grande feature map\n",
    "            target_size = features[0].shape[-2:]  # Assuming features[0] is the largest one\n",
    "\n",
    "            # On fait un interpolate (upsampling) pour chaque feature map pour atteindre la taille cible, puis on les concatène\n",
    "            resized_features = [F.interpolate(f, size=target_size, mode='bilinear', align_corners=False) for f in features]\n",
    "            combined_feature = torch.cat(resized_features, dim=1)\n",
    "         #   print('combined_feature type)', combined_feature.shape)\n",
    "        # Sauvegarder les caractéristiques dans un fichier .pkl\n",
    "        feature_file = os.path.join(output_dir, filename.split('.')[0]+ '.pkl')\n",
    "        with open(feature_file, 'wb') as f:\n",
    "            pickle.dump(combined_feature.cpu().numpy(), f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
